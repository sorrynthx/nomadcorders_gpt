{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Victory Mansions is a building where Winston Smith resides. It is a run-down apartment complex with a faulty elevator, gritty dust, and a hallway that smells of boiled cabbage and old rag mats. The flat where Winston lives is on the seventh floor, and the building is poorly maintained due to the ongoing economy drive in preparation for Hate Week. The building is described as having a gritty and unpleasant atmosphere.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Langchain의 챗 모델과 다양한 문서 로더, 텍스트 분할기, 임베딩, 벡터 스토어 등을 임포트합니다.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# OpenAI의 챗 모델을 낮은 온도 설정과 함께 초기화합니다.\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# 캐시 디렉토리를 설정합니다.\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# 텍스트를 적절한 크기의 청크로 분할하는 분할기를 설정합니다.\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# 비구조화된 파일로부터 문서를 로드합니다.\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "# 로더를 사용하여 문서를 로드하고 분할합니다.\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# OpenAI 임베딩을 초기화합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 캐시를 이용하여 임베딩을 백업합니다.\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "# FAISS 벡터 스토어를 문서로부터 생성합니다.\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# 벡터 스토어를 검색기로 변환합니다.\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "# 대화 프롬프트 템플릿을 설정합니다.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"도우미로서 주어진 문맥을 이용해 질문에 답하십시오. 모르는 경우 모르겠다고만 말하고, 추측하지 마십시오: \\n\\n{context}\"),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"{question}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 체인을 구성하여 문맥, 질문, 추가 정보를 처리한 후 결과를 반환합니다.\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver, \n",
    "        \"question\": RunnablePassthrough(), \n",
    "        \"extra\": RunnablePassthrough()\n",
    "    } \n",
    "    | prompt \n",
    "    | llm\n",
    ")\n",
    "\n",
    "# 체인을 활성화하여 'Victory Mansions'에 대해 설명을 요청합니다.\n",
    "chain.invoke(\"Describe Victory Mansions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. 문서 로더 초기화] -- UnstructuredFileLoader\n",
    "  |\n",
    "  |-> \"./files/chapter_one.docx\" 파일 로드\n",
    "  |   설명: UnstructuredFileLoader는 구조화되지 않은 문서를 로드하는 데 사용됩니다. 여기서는 docx 파일을 로드합니다.\n",
    "  |\n",
    "[2. 텍스트 분할기 초기화] -- CharacterTextSplitter.from_tiktoken_encoder\n",
    "  |   separator=\"\\n\", chunk_size=600, chunk_overlap=100\n",
    "  |   설명: 텍스트를 지정된 크기로 분할하여 처리할 수 있도록 합니다. 여기서는 문자 기반 분할 방식을 사용합니다.\n",
    "  |\n",
    "  |-> 문서 분할\n",
    "  |   설명: 로드된 문서를 위에서 정의한 분할기를 사용해 분할합니다.\n",
    "  |\n",
    "[3. 임베딩 및 캐싱] -- OpenAIEmbeddings & CacheBackedEmbeddings\n",
    "  |\n",
    "  |-> OpenAI 임베딩 생성\n",
    "  |   설명: OpenAI 임베딩을 사용하여 문서의 텍스트를 벡터로 변환합니다.\n",
    "  |\n",
    "  |-> 캐시 디렉토리 설정: LocalFileStore(\"./.cache/\")\n",
    "  |   설명: 임베딩 결과를 캐시하기 위한 로컬 파일 스토어를 설정합니다.\n",
    "  |\n",
    "  |-> 캐시된 임베딩 생성\n",
    "  |   설명: 임베딩을 캐시하여 재사용을 용이하게 하고, 처리 시간을 단축시킵니다.\n",
    "  |\n",
    "[4. 벡터 스토어 및 검색기 초기화] -- FAISS\n",
    "  |\n",
    "  |-> FAISS 벡터 스토어 생성\n",
    "  |   설명: 캐시된 임베딩을 기반으로 FAISS 벡터 스토어를 생성합니다. 이를 통해 효율적인 유사도 검색이 가능해집니다.\n",
    "  |\n",
    "  |-> 검색기로 변환\n",
    "  |   설명: 생성된 벡터 스토어를 사용하여 문서 내에서 정보를 검색할 수 있는 검색기를 초기화합니다.\n",
    "  |\n",
    "[5. 프롬프트 템플릿 및 체인 구성] -- ChatPromptTemplate & Chain\n",
    "  |\n",
    "  |-> 프롬프트 템플릿 설정\n",
    "  |   설명: 사용자 질문과 시스템 응답을 포맷팅하기 위한 템플릿을 설정합니다.\n",
    "  |\n",
    "  |-> 체인 구성\n",
    "  |   설명: 검색 결과, 사용자 질문, 그리고 추가적인 처리를 포함한 전체 처리 과정을 정의하는 체인을 구성합니다.\n",
    "  |\n",
    "[6. 체인 실행 및 질문 처리] -- chain.invoke(\"Describe Victory Mansions\")\n",
    "    설명: 구성된 체인을 사용하여 \"Describe Victory Mansions\"에 대한 질문을 처리하고, 관련 정보를 검색하여 응답을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 Langchain 라이브러리를 사용하여 대화형 프롬프트를 통해 문서의 내용을 기반으로 질문에 답변하는 시스템을 구축하는 과정을 보여줍니다. 여기서는 문서를 로드하고 분할하여 임베딩을 생성한 다음, FAISS를 이용해 벡터 스토어를 만들고, 이를 검색기로 사용합니다. 대화형 프롬프트는 시스템이 검색한 문맥을 기반으로 질문에 답변하도록 유도합니다. 이 과정은 복잡한 체이닝 메커니즘을 통해 실행되며, 결과적으로 사용자의 질문에 대한 답을 제공합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
